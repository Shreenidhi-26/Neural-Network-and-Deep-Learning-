{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**LAB-04 : IMPLEMENTATION OF BACKPROPAGATION**"
      ],
      "metadata": {
        "id": "YcIldXfU4Jcq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEPCDD8l2lwu",
        "outputId": "0fe83337-5373-440c-f582-4daacfbc9cb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1000, Loss: 0.39557039180027914\n",
            "Epoch 2000, Loss: 0.268978650120054\n",
            "Epoch 3000, Loss: 0.20877501367079165\n",
            "Epoch 4000, Loss: 0.17388635703099511\n",
            "Epoch 5000, Loss: 0.15095324126862225\n",
            "Epoch 6000, Loss: 0.13460178571028433\n",
            "Epoch 7000, Loss: 0.12227213430243797\n",
            "Epoch 8000, Loss: 0.11258998461667526\n",
            "Epoch 9000, Loss: 0.10475002443046719\n",
            "Epoch 10000, Loss: 0.09824792584007563\n",
            "Final weights between input and hidden layers: \n",
            " [[0.90571962 7.32475086]\n",
            " [0.90572643 7.32731531]]\n",
            "Final weights between hidden and output layers: \n",
            " [[-27.46354604]\n",
            " [ 21.74964594]]\n",
            "Predicted outputs after training: \n",
            " [[0.05432736]\n",
            " [0.89823995]\n",
            " [0.89824011]\n",
            " [0.13514441]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Activation function: Sigmoid\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Derivative of the sigmoid function\n",
        "def sigmoid_derivative(x):\n",
        "    return x * (1 - x)\n",
        "\n",
        "# Training data (XOR problem)\n",
        "inputs = np.array([[0, 0],\n",
        "                   [0, 1],\n",
        "                   [1, 0],\n",
        "                   [1, 1]])\n",
        "\n",
        "outputs = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Initialize weights randomly with mean 0\n",
        "input_layer_neurons = 2\n",
        "hidden_layer_neurons = 2\n",
        "output_neurons = 1\n",
        "\n",
        "weights_input_hidden = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
        "weights_hidden_output = np.random.uniform(size=(hidden_layer_neurons, output_neurons))\n",
        "\n",
        "# Learning rate\n",
        "learning_rate = 0.5\n",
        "\n",
        "# Number of iterations\n",
        "epochs = 10000\n",
        "\n",
        "# Training process\n",
        "for epoch in range(epochs):\n",
        "    # Forward propagation\n",
        "    hidden_layer_activation = np.dot(inputs, weights_input_hidden)\n",
        "    hidden_layer_output = sigmoid(hidden_layer_activation)\n",
        "\n",
        "    final_layer_activation = np.dot(hidden_layer_output, weights_hidden_output)\n",
        "    predicted_output = sigmoid(final_layer_activation)\n",
        "\n",
        "    # Compute the error\n",
        "    error = outputs - predicted_output\n",
        "\n",
        "    # Backpropagation\n",
        "    d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
        "\n",
        "    error_hidden_layer = d_predicted_output.dot(weights_hidden_output.T)\n",
        "    d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
        "\n",
        "    # Updating the weights\n",
        "    weights_hidden_output += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
        "    weights_input_hidden += inputs.T.dot(d_hidden_layer) * learning_rate\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "        loss = np.mean(np.abs(error))\n",
        "        print(f'Epoch {epoch + 1}, Loss: {loss}')\n",
        "\n",
        "# Testing the trained neural network\n",
        "print(\"Final weights between input and hidden layers: \\n\", weights_input_hidden)\n",
        "print(\"Final weights between hidden and output layers: \\n\", weights_hidden_output)\n",
        "print(\"Predicted outputs after training: \\n\", predicted_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:**\n",
        "\n",
        "The model's performance improved significantly as the loss decreased from 0.3956 to 0.0982 over 10000 epochs. The final weights between layers show how the inputs are combined and transformed. The predicted outputs indicate that the model can effectively distinguish between different input patterns, demonstrating its ability to make accurate predictions."
      ],
      "metadata": {
        "id": "CybGj2yr3vc6"
      }
    }
  ]
}